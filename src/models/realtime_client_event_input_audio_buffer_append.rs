/*
 * OpenAI API
 *
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.3.0
 *
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

/// RealtimeClientEventInputAudioBufferAppend : Send this event to append audio bytes to the input audio buffer. The audio  buffer is temporary storage you can write to and later commit. A \"commit\" will create a new user message item in the conversation history from the buffer content and clear the buffer. Input audio transcription (if enabled) will be generated when the buffer is committed.  If VAD is enabled the audio buffer is used to detect speech and the server will decide  when to commit. When Server VAD is disabled, you must commit the audio buffer manually. Input audio noise reduction operates on writes to the audio buffer.  The client may choose how much audio to place in each event up to a maximum  of 15 MiB, for example streaming smaller chunks from the client may allow the  VAD to be more responsive. Unlike most other client events, the server will  not send a confirmation response to this event.
#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize, bon::Builder)]
pub struct RealtimeClientEventInputAudioBufferAppend {
    /// Optional client-generated ID used to identify this event.
    #[serde(rename = "event_id", skip_serializing_if = "Option::is_none")]
    pub event_id: Option<String>,
    /// The event type, must be `input_audio_buffer.append`.
    #[serde(rename = "type")]
    pub r#type: Type,
    /// Base64-encoded audio bytes. This must be in the format specified by the  `input_audio_format` field in the session configuration.
    #[serde(rename = "audio")]
    pub audio: String,
}

impl RealtimeClientEventInputAudioBufferAppend {
    /// Send this event to append audio bytes to the input audio buffer. The audio  buffer is temporary storage you can write to and later commit. A \"commit\" will create a new user message item in the conversation history from the buffer content and clear the buffer. Input audio transcription (if enabled) will be generated when the buffer is committed.  If VAD is enabled the audio buffer is used to detect speech and the server will decide  when to commit. When Server VAD is disabled, you must commit the audio buffer manually. Input audio noise reduction operates on writes to the audio buffer.  The client may choose how much audio to place in each event up to a maximum  of 15 MiB, for example streaming smaller chunks from the client may allow the  VAD to be more responsive. Unlike most other client events, the server will  not send a confirmation response to this event.
    pub fn new(r#type: Type, audio: String) -> RealtimeClientEventInputAudioBufferAppend {
        RealtimeClientEventInputAudioBufferAppend {
            event_id: None,
            r#type,
            audio,
        }
    }
}
/// The event type, must be `input_audio_buffer.append`.
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]
pub enum Type {
    #[serde(rename = "input_audio_buffer.append")]
    InputAudioBufferAppend,
}

impl Default for Type {
    fn default() -> Type {
        Self::InputAudioBufferAppend
    }
}

impl std::fmt::Display for RealtimeClientEventInputAudioBufferAppend {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match serde_json::to_string(self) {
            Ok(s) => write!(f, "{}", s),
            Err(_) => Err(std::fmt::Error),
        }
    }
}
