/*
 * OpenAI API
 *
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.3.0
 *
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize, bon::Builder)]
pub struct EvalTestingCriteriaInner {
    /// The object type, which is always `label_model`.
    #[serde(rename = "type")]
    pub r#type: Type,
    /// The name of the grader.
    #[serde(rename = "name")]
    pub name: String,
    /// ID of the model to use
    #[serde(rename = "model")]
    pub model: String,
    /// The input text. This may include template strings.
    #[serde(rename = "input")]
    pub input: Vec<models::EvalItem>,
    /// The labels to assign to each item in the evaluation.
    #[serde(rename = "labels")]
    pub labels: Vec<String>,
    /// The labels that indicate a passing result. Must be a subset of labels.
    #[serde(rename = "passing_labels")]
    pub passing_labels: Vec<String>,
    /// The text being graded against.
    #[serde(rename = "reference")]
    pub reference: String,
    /// The string check operation to perform. One of `eq`, `ne`, `like`, or `ilike`.
    #[serde(rename = "operation")]
    pub operation: Operation,
    /// The evaluation metric to use. One of `cosine`, `fuzzy_match`, `bleu`,  `gleu`, `meteor`, `rouge_1`, `rouge_2`, `rouge_3`, `rouge_4`, `rouge_5`,  or `rouge_l`.
    #[serde(rename = "evaluation_metric")]
    pub evaluation_metric: EvaluationMetric,
    /// The threshold for the score.
    #[serde(rename = "pass_threshold")]
    pub pass_threshold: f64,
    /// The source code of the python script.
    #[serde(rename = "source")]
    pub source: String,
    /// The image tag to use for the python script.
    #[serde(rename = "image_tag", skip_serializing_if = "Option::is_none")]
    pub image_tag: Option<String>,
    #[serde(rename = "sampling_params", skip_serializing_if = "Option::is_none")]
    pub sampling_params: Option<Box<models::GraderScoreModelSamplingParams>>,
    /// The range of the score. Defaults to `[0, 1]`.
    #[serde(rename = "range", skip_serializing_if = "Option::is_none")]
    pub range: Option<Vec<f64>>,
}

impl EvalTestingCriteriaInner {
    pub fn new(
        r#type: Type,
        name: String,
        model: String,
        input: Vec<models::EvalItem>,
        labels: Vec<String>,
        passing_labels: Vec<String>,
        reference: String,
        operation: Operation,
        evaluation_metric: EvaluationMetric,
        pass_threshold: f64,
        source: String,
    ) -> EvalTestingCriteriaInner {
        EvalTestingCriteriaInner {
            r#type,
            name,
            model,
            input,
            labels,
            passing_labels,
            reference,
            operation,
            evaluation_metric,
            pass_threshold,
            source,
            image_tag: None,
            sampling_params: None,
            range: None,
        }
    }
}
/// The object type, which is always `label_model`.
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]
pub enum Type {
    #[serde(rename = "label_model")]
    LabelModel,
    #[serde(rename = "string_check")]
    StringCheck,
    #[serde(rename = "text_similarity")]
    TextSimilarity,
    #[serde(rename = "python")]
    Python,
    #[serde(rename = "score_model")]
    ScoreModel,
}

impl Default for Type {
    fn default() -> Type {
        Self::LabelModel
    }
}
/// The string check operation to perform. One of `eq`, `ne`, `like`, or `ilike`.
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]
pub enum Operation {
    #[serde(rename = "eq")]
    Eq,
    #[serde(rename = "ne")]
    Ne,
    #[serde(rename = "like")]
    Like,
    #[serde(rename = "ilike")]
    Ilike,
}

impl Default for Operation {
    fn default() -> Operation {
        Self::Eq
    }
}
/// The evaluation metric to use. One of `cosine`, `fuzzy_match`, `bleu`,  `gleu`, `meteor`, `rouge_1`, `rouge_2`, `rouge_3`, `rouge_4`, `rouge_5`,  or `rouge_l`.
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]
pub enum EvaluationMetric {
    #[serde(rename = "cosine")]
    Cosine,
    #[serde(rename = "fuzzy_match")]
    FuzzyMatch,
    #[serde(rename = "bleu")]
    Bleu,
    #[serde(rename = "gleu")]
    Gleu,
    #[serde(rename = "meteor")]
    Meteor,
    #[serde(rename = "rouge_1")]
    Rouge1,
    #[serde(rename = "rouge_2")]
    Rouge2,
    #[serde(rename = "rouge_3")]
    Rouge3,
    #[serde(rename = "rouge_4")]
    Rouge4,
    #[serde(rename = "rouge_5")]
    Rouge5,
    #[serde(rename = "rouge_l")]
    RougeL,
}

impl Default for EvaluationMetric {
    fn default() -> EvaluationMetric {
        Self::Cosine
    }
}

impl std::fmt::Display for EvalTestingCriteriaInner {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match serde_json::to_string(self) {
            Ok(s) => write!(f, "{}", s),
            Err(_) => Err(std::fmt::Error),
        }
    }
}
