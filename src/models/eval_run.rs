/*
 * OpenAI API
 *
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.3.0
 *
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

/// EvalRun : A schema representing an evaluation run.
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize, bon::Builder)]
pub struct EvalRun {
    /// The type of the object. Always \"eval.run\".
    #[serde(rename = "object")]
    pub object: Object,
    /// Unique identifier for the evaluation run.
    #[serde(rename = "id")]
    pub id: String,
    /// The identifier of the associated evaluation.
    #[serde(rename = "eval_id")]
    pub eval_id: String,
    /// The status of the evaluation run.
    #[serde(rename = "status")]
    pub status: String,
    /// The model that is evaluated, if applicable.
    #[serde(rename = "model")]
    pub model: String,
    /// The name of the evaluation run.
    #[serde(rename = "name")]
    pub name: String,
    /// Unix timestamp (in seconds) when the evaluation run was created.
    #[serde(rename = "created_at")]
    pub created_at: i32,
    /// The URL to the rendered evaluation run report on the UI dashboard.
    #[serde(rename = "report_url")]
    pub report_url: String,
    #[serde(rename = "result_counts")]
    pub result_counts: Box<models::EvalRunResultCounts>,
    /// Usage statistics for each model during the evaluation run.
    #[serde(rename = "per_model_usage")]
    pub per_model_usage: Vec<models::EvalRunPerModelUsageInner>,
    /// Results per testing criteria applied during the evaluation run.
    #[serde(rename = "per_testing_criteria_results")]
    pub per_testing_criteria_results: Vec<models::EvalRunPerTestingCriteriaResultsInner>,
    #[serde(rename = "data_source")]
    pub data_source: Box<models::EvalRunDataSource>,
    /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.
    #[serde(rename = "metadata", deserialize_with = "Option::deserialize")]
    pub metadata: Option<std::collections::HashMap<String, String>>,
    #[serde(rename = "error")]
    pub error: Box<models::EvalApiError>,
}

impl EvalRun {
    /// A schema representing an evaluation run.
    pub fn new(
        object: Object,
        id: String,
        eval_id: String,
        status: String,
        model: String,
        name: String,
        created_at: i32,
        report_url: String,
        result_counts: models::EvalRunResultCounts,
        per_model_usage: Vec<models::EvalRunPerModelUsageInner>,
        per_testing_criteria_results: Vec<models::EvalRunPerTestingCriteriaResultsInner>,
        data_source: models::EvalRunDataSource,
        metadata: Option<std::collections::HashMap<String, String>>,
        error: models::EvalApiError,
    ) -> EvalRun {
        EvalRun {
            object,
            id,
            eval_id,
            status,
            model,
            name,
            created_at,
            report_url,
            result_counts: Box::new(result_counts),
            per_model_usage,
            per_testing_criteria_results,
            data_source: Box::new(data_source),
            metadata,
            error: Box::new(error),
        }
    }
}
/// The type of the object. Always \"eval.run\".
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]
pub enum Object {
    #[serde(rename = "eval.run")]
    EvalRun,
}

impl Default for Object {
    fn default() -> Object {
        Self::EvalRun
    }
}

impl std::fmt::Display for EvalRun {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match serde_json::to_string(self) {
            Ok(s) => write!(f, "{}", s),
            Err(_) => Err(std::fmt::Error),
        }
    }
}
