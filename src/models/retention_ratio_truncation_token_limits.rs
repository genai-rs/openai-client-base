/*
 * OpenAI API
 *
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.3.0
 *
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

/// RetentionRatioTruncationTokenLimits : Optional custom token limits for this truncation strategy. If not provided, the model's default token limits will be used.
#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize, bon::Builder)]
pub struct RetentionRatioTruncationTokenLimits {
    /// Maximum tokens allowed in the conversation after instructions (which including tool definitions). For example, setting this to 5,000 would mean that truncation would occur when the conversation exceeds 5,000 tokens after instructions. This cannot be higher than the model's context window size minus the maximum output tokens.
    #[serde(rename = "post_instructions", skip_serializing_if = "Option::is_none")]
    pub post_instructions: Option<i32>,
}

impl RetentionRatioTruncationTokenLimits {
    /// Optional custom token limits for this truncation strategy. If not provided, the model's default token limits will be used.
    pub fn new() -> RetentionRatioTruncationTokenLimits {
        RetentionRatioTruncationTokenLimits {
            post_instructions: None,
        }
    }
}

impl std::fmt::Display for RetentionRatioTruncationTokenLimits {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match serde_json::to_string(self) {
            Ok(s) => write!(f, "{}", s),
            Err(_) => Err(std::fmt::Error),
        }
    }
}
